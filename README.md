![Python](https://img.shields.io/badge/Python-3.10+-blue)
![LangChain](https://img.shields.io/badge/LangChain-0.3+-green)
![FAISS](https://img.shields.io/badge/VectorDB-FAISS-orange)
![Ollama](https://img.shields.io/badge/LLM-Ollama-purple)
![Streamlit](https://img.shields.io/badge/UI-Streamlit-red)
![RAG](https://img.shields.io/badge/Architecture-RAG-success)
![Offline](https://img.shields.io/badge/Mode-Offline-important)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen)

# ğŸ“š Offline Research Paper Assistant (RAG + FAISS + Ollama)

An **offline ChatGPT-style AI assistant** that allows users to upload multiple research papers (PDFs), build semantic embeddings locally, and ask document-grounded questions using Retrieval-Augmented Generation (RAG).

Runs completely **offline** using:

â€¢ FAISS for vector search  
â€¢ Ollama for local LLM inference  
â€¢ Streamlit for Chat UI  

---

## ğŸ“‘ Table of Contents

- Project Overview  
- Key Features  
- Tech Stack  
- Folder Structure  
- Installation  
- How It Works  
- Architecture
- Limitations  
- Future Improvements  

---

## ğŸš€ Project Overview

This project enables semantic search and Q&A over multiple PDFs using RAG architecture.

Users can:

â€¢ Upload up to 5 PDFs  
â€¢ Build vector embeddings locally  
â€¢ Ask questions conversationally  
â€¢ Maintain chat history  
â€¢ Export conversations  
â€¢ Clear chat  
â€¢ Run fully offline  

Designed for:

âœ” Research papers  
âœ” Technical documents  
âœ” PDFs  

---

## âœ¨ Key Features

âœ… Multi-PDF Support (up to 5 files)  
âœ… Offline RAG Pipeline  
âœ… FAISS Vector Database  
âœ… Ollama Local LLM  
âœ… ChatGPT-style UI  
âœ… Conversation Memory  
âœ… Sidebar Upload  
âœ… JSON Chat History  
âœ… Clear Chat Option

---

## ğŸ§  Tech Stack

- Python  
- LangChain  
- FAISS  
- Ollama  
- Streamlit  
- SentenceTransformers  
- HuggingFace Embeddings  

---

## ğŸ“ Folder Structure
PDF_Reader/\
â”‚
â”œâ”€â”€ app.py\
â”œâ”€â”€ ingest.py\
â”œâ”€â”€ rag_chain.py\
â”œâ”€â”€ requirements.txt\
â””â”€â”€ README.md


---

## âš™ Installation

1ï¸âƒ£ Create Environment

2ï¸âƒ£ Install Dependencies
```bash 
pip install -r requirements.txt 
```

3ï¸âƒ£ Install Ollama

Download Ollama:

https://ollama.com

Then pull model:

```bash
ollama pull llama3
```

Build Vector Index

```bash
python ingest.py
```

Launch App

```bash 
streamlit run app.py
```
---
## ğŸ”„ How It Works

Upload PDFs (up to 5 files) via Streamlit sidebar or place them manually in the data folder.

Run ingest.py â†’ creates FAISS vectorstore

After adding new PDFs, run ingest.py to rebuild the vector database.

Streamlit UI loads vectorstore

Questions retrieved + answered via Ollama

## â–¶ Architecture:

PDF â†’ Chunking â†’ Embeddings â†’ FAISS â†’ Retriever â†’ Ollama â†’ Answer

---

## âš  Limitations

â€¢ Small LLM models may sometimes hallucinate\
â€¢ Requires re-running ingest.py after adding PDFs

---
## ğŸ”® Future Improvements

Citation highlighting

Hybrid search (BM25 + FAISS)

## ğŸ‘©â€ğŸ’» Author

Shreya Sidabache

